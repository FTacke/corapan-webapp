# CO.RA.PAN Database Performance Analyse

**Datum:** 17. Oktober 2025  
**Analysierte Version:** CO.RA.PAN-WEB_new  
**Datenbank:** transcription.db (348.97 MB, 1,351,207 Tokens)

---

## üìä Executive Summary

Die Datenbankinteraktionen der CO.RA.PAN-Webapp sind **grunds√§tzlich gut optimiert**, es gibt jedoch **mehrere kritische Performance-Probleme**, die behoben werden sollten:

### üî¥ Kritische Probleme
1. **ALL RESULTS werden IMMER geladen** (~1.3M Zeilen bei Volltextsuche) 
2. **LIKE-Queries ohne Index** (5.2s f√ºr einfache Suche)
3. **Fehlende Composite Indexes** f√ºr h√§ufige Filterkombinationen
4. **Fehlende Statistiken** (ANALYZE wurde nie ausgef√ºhrt)

### üü¢ Gut gemacht
1. Token-ID Lookups sind schnell (4ms dank UNIQUE constraint)
2. Datenbankstruktur ist normalisiert und sauber
3. Prepared Statements werden korrekt verwendet
4. Context Manager f√ºr Connections

---

## üîç Detaillierte Performance-Messungen

### Test-Ergebnisse (auf 1.3M Tokens)

| Test | Ergebnis | Zeit | Query Plan | Status |
|------|----------|------|------------|--------|
| LIKE '%casa%' | 744 | **5.182s** | SCAN tokens | üî¥ KRITISCH |
| Exact 'casa' | 276 | 0.345s | SCAN tokens | üü° SUBOPTIMAL |
| Token-ID Lookup | 1 | 0.004s | INDEX (token_id) | ‚úÖ OPTIMAL |
| Combined Filters | 0 | 0.314s | SCAN tokens | üü° SUBOPTIMAL |
| Two-word JOIN | 1,202 | 0.360s | SCAN + PK | üü° AKZEPTABEL |
| ORDER BY + LIMIT | 25 | 0.391s | SCAN tokens | üü° SUBOPTIMAL |

### Datenbank-Statistiken

```
Database size:       348.97 MB
Total tokens:        1,351,207
Total files:         132
Countries:           24
```

### Filter-Selektivit√§t

| Spalte | Distinct Values | Selektivit√§t | Index? |
|--------|-----------------|--------------|--------|
| `country_code` | 24 | 0.002% | ‚ùå NEIN |
| `speaker_type` | 3 | 0.000% | ‚ùå NEIN |
| `sex` | 3 | 0.000% | ‚ùå NEIN |
| `mode` | 5 | 0.000% | ‚ùå NEIN |
| `discourse` | 4 | 0.000% | ‚ùå NEIN |
| `filename` | 132 | 0.010% | ‚ùå NEIN |
| `token_id` | ~1.35M | 100% | ‚úÖ JA |

---

## üö® Problem #1: ALL RESULTS werden immer geladen

### Aktuelles Verhalten (corpus_search.py, Zeilen 221-226)

```python
# Paginierte Daten (nur 25 Zeilen)
cursor.execute(data_sql, bindings_for_data)
rows = cursor.fetchall()  # 25 rows

# ABER: Alle Ergebnisse werden AUCH geladen!
all_sql = (
    "SELECT * FROM (" + sql_words + ") AS base "
    f"WHERE 1=1{filter_clause} ORDER BY {order_sql_full}"
)
cursor.execute(all_sql, bindings_for_all)
all_rows = cursor.fetchall()  # ‚Üê KANN 100.000+ ZEILEN SEIN!
```

### Impact

Bei einer Suche nach `"la"` (46,666 Vorkommen):
- **Paginierte Query:** 25 Zeilen ‚Üí ~0.4s
- **ALL Query:** 46,666 Zeilen ‚Üí **~15-20 Sekunden** + massive Speichernutzung

### Verwendung von `all_items`

```python
# In corpus_search.py:
unique_countries = len({row["country_code"] for row in all_row_dicts})
unique_files = len({row["filename"] for row in all_row_dicts})

# In corpus.py (Route):
context.update({
    "all_results": service_result["all_items_legacy"],  # ‚Üê Frontend bekommt ALLE
    ...
})
```

**Frontend:** DataTables bekommt ALLE Ergebnisse client-side, was bei gro√üen Resultsets den Browser einfriert.

### üéØ Empfohlene L√∂sung

**Option A: Statistiken separat berechnen (EMPFOHLEN)**
```python
# Statt alle Zeilen zu laden:
cursor.execute(f"""
    SELECT 
        COUNT(DISTINCT country_code) as countries,
        COUNT(DISTINCT filename) as files
    FROM ({sql_words}) AS base
    WHERE 1=1{filter_clause}
""", bindings_for_count)
stats = cursor.fetchone()
```

**Option B: Server-Side Pagination f√ºr DataTables**
- DataTables unterst√ºtzt Server-Side Processing
- Nur die aktuell sichtbaren Zeilen werden geladen
- Bessere Performance bei gro√üen Datasets

**Gesch√§tzter Performance-Gewinn:** **10-20x schneller** bei h√§ufigen W√∂rtern

---

## üö® Problem #2: Fehlende Indexes f√ºr Text-Suchen

### Aktueller Zustand

```sql
-- Nur 2 Indexes vorhanden:
CREATE INDEX idx_tokens_token_id ON tokens(token_id);  -- Redundant mit UNIQUE
-- sqlite_autoindex_tokens_1 (UNIQUE auf token_id)
```

### Problem

**ALLE** Queries machen einen **FULL TABLE SCAN** auf 1.3M Zeilen:
- `WHERE text LIKE '%casa%'` ‚Üí SCAN (5.2s)
- `WHERE text = 'casa'` ‚Üí SCAN (0.345s)  
- `WHERE country_code = 'AR'` ‚Üí SCAN (0.314s)

### üéØ Empfohlene Indexes

```sql
-- 1. Text-Suchen (exakt)
CREATE INDEX idx_tokens_text ON tokens(text);
-- Nutzen: Exakte Suchen ~100x schneller
-- Nachteil: Hilft NICHT bei LIKE '%word%'

-- 2. H√§ufige Filter
CREATE INDEX idx_tokens_country ON tokens(country_code);
CREATE INDEX idx_tokens_speaker ON tokens(speaker_type);
CREATE INDEX idx_tokens_mode ON tokens(mode);

-- 3. Composite Index f√ºr h√§ufige Kombinationen
CREATE INDEX idx_tokens_country_speaker_mode 
ON tokens(country_code, speaker_type, mode);
-- Nutzen: Kombinierte Filter 10-100x schneller

-- 4. Multi-Word Sequences
CREATE INDEX idx_tokens_filename_id ON tokens(filename, id);
-- Nutzen: JOIN f√ºr Wortfolgen ~5x schneller

-- 5. Lemma-Suchen
CREATE INDEX idx_tokens_lemma ON tokens(lemma);
```

### Full-Text Search (FTS5) - F√ºr LIKE Queries

**Problem:** `LIKE '%word%'` kann NIE einen Index nutzen (SQLite-Limitation)

**L√∂sung:** SQLite FTS5 Virtual Table

```sql
-- FTS5 Virtual Table erstellen
CREATE VIRTUAL TABLE tokens_fts USING fts5(
    token_id UNINDEXED,
    text,
    lemma,
    content='tokens',
    content_rowid='id'
);

-- Trigger f√ºr Auto-Update
CREATE TRIGGER tokens_ai AFTER INSERT ON tokens BEGIN
  INSERT INTO tokens_fts(rowid, token_id, text, lemma) 
  VALUES (new.id, new.token_id, new.text, new.lemma);
END;
```

**Query-√Ñnderung:**
```python
# Alt (5.2s):
SELECT * FROM tokens WHERE text LIKE '%casa%'

# Neu mit FTS5 (<0.1s):
SELECT t.* FROM tokens t
JOIN tokens_fts fts ON fts.rowid = t.id
WHERE tokens_fts MATCH 'casa'
```

**Gesch√§tzter Performance-Gewinn:** **50-100x schneller** f√ºr Text-Suchen

---

## üö® Problem #3: Keine Datenbank-Statistiken

### Aktueller Zustand

```
sqlite> SELECT * FROM sqlite_stat1;
-- Error: no such table: sqlite_stat1
```

**ANALYZE wurde NIE ausgef√ºhrt!**

### Impact

Ohne Statistiken kann der SQLite Query Planner nicht optimal entscheiden:
- Welchen Index nutzen?
- Welche JOIN-Reihenfolge?
- Welche Query-Strategie?

### üéØ L√∂sung

```sql
-- Nach jedem DB-Update ausf√ºhren:
ANALYZE;
```

**In database_creation.py erg√§nzen:**
```python
def run_tokens():
    # ... existing code ...
    conn_trans.commit()
    
    # NEU: Statistiken generieren
    print("Generiere Datenbank-Statistiken...")
    c_trans.execute("ANALYZE")
    conn_trans.commit()
    
    conn_trans.close()
```

**Gesch√§tzter Performance-Gewinn:** **5-20%** bei komplexen Queries

---

## üö® Problem #4: Datenbankverbindungen nicht wiederverwendet

### Aktueller Code (database.py)

```python
@contextmanager
def open_db(name: str) -> Iterator[sqlite3.Connection]:
    """Context manager that closes the connection automatically."""
    connection = get_connection(name)  # ‚Üê Neue Connection bei JEDEM Request!
    try:
        yield connection
    finally:
        connection.close()  # ‚Üê Connection wird sofort geschlossen
```

### Problem

Bei jeder Suchanfrage wird:
1. Neue SQLite-Connection ge√∂ffnet
2. DB-File ge√∂ffnet
3. Locks erworben
4. Connection geschlossen

**Overhead:** ~5-10ms pro Request

### üéØ Empfohlene L√∂sung: Connection Pool

```python
# Neu: Connection Pool mit Flask-g
from flask import g

def get_db_connection(name: str) -> sqlite3.Connection:
    """Get or create a connection for this request context."""
    if not hasattr(g, 'db_connections'):
        g.db_connections = {}
    
    if name not in g.db_connections:
        g.db_connections[name] = get_connection(name)
    
    return g.db_connections[name]

@app.teardown_appcontext
def close_db_connections(error):
    """Close all DB connections at end of request."""
    if hasattr(g, 'db_connections'):
        for conn in g.db_connections.values():
            conn.close()
```

**Gesch√§tzter Performance-Gewinn:** **5-10ms** pro Request

---

## üìà Optimierungsvorschl√§ge - Priorisiert

### üî¥ Priorit√§t 1: KRITISCH (Sofort umsetzen)

1. **ALL RESULTS entfernen**
   - Zeile 221-226 in `corpus_search.py`
   - Separate COUNT DISTINCT f√ºr Statistiken
   - **Gewinn:** 10-20x schneller bei h√§ufigen W√∂rtern
   - **Aufwand:** 1-2 Stunden

2. **ANALYZE ausf√ºhren**
   - In `database_creation.py` erg√§nzen
   - **Gewinn:** 5-20% bei komplexen Queries
   - **Aufwand:** 5 Minuten

3. **Composite Index f√ºr Filter**
   - `CREATE INDEX idx_tokens_country_speaker_mode`
   - **Gewinn:** 10-100x bei gefilterten Suchen
   - **Aufwand:** 10 Minuten

### üü° Priorit√§t 2: HOCH (Diese Woche)

4. **FTS5 f√ºr Text-Suchen**
   - Virtual Table + Triggers
   - Query-√Ñnderungen in `corpus_search.py`
   - **Gewinn:** 50-100x bei LIKE-Queries
   - **Aufwand:** 3-4 Stunden

5. **Einzelne Filter-Indexes**
   - Indexes f√ºr `text`, `lemma`, `country_code`
   - **Gewinn:** 10-50x bei einfachen Filtern
   - **Aufwand:** 30 Minuten

6. **Connection Pooling**
   - Flask-g basiertes Pooling
   - **Gewinn:** 5-10ms pro Request
   - **Aufwand:** 1 Stunde

### üü¢ Priorit√§t 3: MITTEL (N√§chster Sprint)

7. **Server-Side Pagination f√ºr DataTables**
   - API-√Ñnderung f√ºr schrittweise Datenabruf
   - **Gewinn:** Bessere UX bei gro√üen Resultsets
   - **Aufwand:** 4-6 Stunden

8. **Query Result Caching**
   - Redis/Memcached f√ºr h√§ufige Queries
   - **Gewinn:** ~100x bei wiederholten Suchen
   - **Aufwand:** 2-3 Stunden

9. **PRAGMA Optimierungen**
   - `PRAGMA cache_size = -64000` (64MB Cache)
   - `PRAGMA temp_store = MEMORY`
   - `PRAGMA journal_mode = WAL`
   - **Gewinn:** 10-30% generell
   - **Aufwand:** 15 Minuten

---

## üìã Implementierungsplan

### Phase 1: Quick Wins (1 Tag)

```python
# 1. database_creation.py - ANALYZE hinzuf√ºgen
def run_tokens():
    # ... existing code ...
    conn_trans.commit()
    print("Generating database statistics...")
    c_trans.execute("ANALYZE")
    conn_trans.commit()

# 2. database_creation.py - Indexes hinzuf√ºgen
def create_indexes(conn):
    cursor = conn.cursor()
    indexes = [
        "CREATE INDEX IF NOT EXISTS idx_tokens_text ON tokens(text)",
        "CREATE INDEX IF NOT EXISTS idx_tokens_lemma ON tokens(lemma)",
        "CREATE INDEX IF NOT EXISTS idx_tokens_country ON tokens(country_code)",
        "CREATE INDEX IF NOT EXISTS idx_tokens_country_speaker_mode ON tokens(country_code, speaker_type, mode)",
        "CREATE INDEX IF NOT EXISTS idx_tokens_filename_id ON tokens(filename, id)",
    ]
    for idx_sql in indexes:
        print(f"Creating index: {idx_sql}")
        cursor.execute(idx_sql)
    conn.commit()
```

### Phase 2: ALL RESULTS Fix (2 Stunden)

```python
# corpus_search.py - Statistiken separat berechnen
def search_tokens(params: SearchParams) -> dict[str, object]:
    # ... existing code ...
    
    # ENTFERNEN: all_sql, all_rows
    # ERSETZEN mit:
    stats_sql = f"""
        SELECT 
            COUNT(DISTINCT country_code) as countries,
            COUNT(DISTINCT filename) as files
        FROM ({sql_words if sql_words else 'SELECT * FROM tokens'}) AS base
        WHERE 1=1{filter_clause}
    """
    cursor.execute(stats_sql, bindings_for_count)
    stats = cursor.fetchone()
    
    return {
        "items": row_dicts,
        "items_legacy": [_to_legacy_row(row) for row in row_dicts],
        # ENTFERNEN: "all_items", "all_items_legacy"
        "total": total_results,
        "unique_countries": stats[0],
        "unique_files": stats[1],
        # ...
    }
```

### Phase 3: FTS5 Integration (4 Stunden)

```python
# database_creation.py - FTS5 erstellen
def create_fts_table(conn):
    cursor = conn.cursor()
    
    # Drop existing
    cursor.execute("DROP TABLE IF EXISTS tokens_fts")
    
    # Create FTS5
    cursor.execute("""
        CREATE VIRTUAL TABLE tokens_fts USING fts5(
            token_id UNINDEXED,
            text,
            lemma,
            content='tokens',
            content_rowid='id'
        )
    """)
    
    # Populate
    cursor.execute("""
        INSERT INTO tokens_fts(rowid, token_id, text, lemma)
        SELECT id, token_id, text, lemma FROM tokens
    """)
    
    conn.commit()

# corpus_search.py - FTS5 Queries
def _build_word_query_fts(words: list[str], column: str, exact: bool):
    if column not in ('text', 'lemma'):
        return _build_word_query(words, column, exact)  # Fallback
    
    # FTS5 Match Query
    if len(words) == 1:
        match_expr = f'"{words[0]}"' if exact else words[0]
    else:
        match_expr = ' '.join(f'"{w}"' if exact else w for w in words)
    
    sql = f"""
        SELECT t.* FROM tokens t
        JOIN tokens_fts fts ON fts.rowid = t.id
        WHERE tokens_fts MATCH ?
    """
    return sql, [match_expr]
```

---

## üß™ Testing-Strategie

### Performance Benchmarks

```python
# tests/test_performance.py
import time

def test_search_performance():
    # H√§ufiges Wort (sollte <1s sein nach Optimierung)
    start = time.time()
    result = search_tokens(SearchParams(query="la", page_size=25))
    elapsed = time.time() - start
    assert elapsed < 1.0, f"Search too slow: {elapsed}s"
    
    # Seltenes Wort (sollte <0.5s sein)
    start = time.time()
    result = search_tokens(SearchParams(query="xenofobia", page_size=25))
    elapsed = time.time() - start
    assert elapsed < 0.5, f"Search too slow: {elapsed}s"
    
    # Kombinierte Filter (sollte <0.3s sein)
    start = time.time()
    result = search_tokens(SearchParams(
        query="casa",
        countries=["AR"],
        speaker_types=["pro"],
        page_size=25
    ))
    elapsed = time.time() - start
    assert elapsed < 0.3, f"Filtered search too slow: {elapsed}s"
```

---

## üìä Erwartete Gesamt-Performance-Verbesserung

| Szenario | Vorher | Nachher | Verbesserung |
|----------|--------|---------|--------------|
| H√§ufiges Wort ("la") | ~20s | <1s | **20x** |
| Seltenes Wort ("casa") | 5.2s | <0.1s | **50x** |
| Token-ID Lookup | 0.004s | 0.004s | ‚úÖ Bereits optimal |
| Gefilterte Suche | 5.5s | <0.5s | **11x** |
| Wortfolge (2 W√∂rter) | 5.5s | <0.2s | **27x** |

**Gesamtnutzen:** Suchen werden im Durchschnitt **20-50x schneller**

---

## ‚úÖ Was bereits gut ist

1. **Token-ID System** - Eindeutig, schnell, gut designed
2. **Prepared Statements** - SQL Injection gesch√ºtzt
3. **Context Manager** - Sauberes Resource Management
4. **Datenbankstruktur** - Normalisiert, keine Redundanz
5. **Pagination** - Grunds√§tzlich implementiert (nur all_results problematisch)
6. **Trennung Public/Private DB** - Gute Security-Praxis

---

## üéØ Empfohlene N√§chste Schritte

1. **Sofort** (heute):
   - ANALYZE ausf√ºhren
   - Composite Index erstellen
   
2. **Diese Woche**:
   - ALL RESULTS entfernen
   - FTS5 implementieren
   
3. **N√§chster Sprint**:
   - Server-Side Pagination
   - Connection Pooling

---

## üìö Referenzen

- SQLite FTS5: https://www.sqlite.org/fts5.html
- SQLite Query Planner: https://www.sqlite.org/queryplanner.html
- SQLite Performance: https://www.sqlite.org/pragma.html
- DataTables Server-Side: https://datatables.net/manual/server-side

---

**Erstellt:** 2025-10-17  
**Autor:** Performance Analysis Tool  
**Status:** ‚úÖ Ready for Implementation
